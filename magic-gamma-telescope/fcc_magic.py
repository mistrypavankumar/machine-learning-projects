# -*- coding: utf-8 -*-
"""fcc-MAGIC.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1W11wi2kmdBucit-uY1TcLCENUB9JNyIW
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler

# for random data values
from imblearn.over_sampling import RandomOverSampler

"""# **Dataset Information**

The data set described simulates the detection of high-energy gamma particles by a ground-based atmospheric Cherenkov gamma telescope. This telescope uses the imaging technique to observe high-energy gamma rays, capitalizing on Cherenkov radiation emitted by charged particles in the electromagnetic showers triggered by gamma rays in the atmosphere. The radiation, in visible to UV wavelengths, is recorded by photomultiplier tubes in the telescope's camera.

The patterns formed by Cherenkov photons, varying from hundreds to about 10,000, help distinguish primary gamma ray showers from cosmic ray-induced hadronic showers. Key features include elongated clusters in shower images and parameters like the Hillas parameters, derived from principal component analysis, to identify the showers. Asymmetry in energy depositions and the extent of clusters also aid in discrimination. The dataset was generated using the Monte Carlo program CORSIKA.
"""

# we don't have columns name so we are creating by ourself
cols = ['fLength', 'fWidth', 'rSize', 'fConc', 'fConc1', 'fAsym', 'fM3Long','fM3Trans', 'fAlpha', 'fDist', 'class']
df = pd.read_csv("magic04.data", names = cols)

df.head()

df['class'].unique()

# lets convert g = 1 and h = 0
df['class'] = (df['class'] == 'g').astype(int)

df['class'].unique()

df.head()

for label in cols[:-1]:
  plt.hist(df[df['class'] == 1][label], color = "blue", label = "gamma", alpha=0.7, density = True)
  plt.hist(df[df['class'] == 0][label], color = "red", label = "hadron", alpha = 0.7, density = True)
  plt.title(label)
  plt.ylabel("Probability")
  plt.xlabel(label)
  plt.show()

"""# Train, Validation, test datasets"""

train, valid, test = np.split(df.sample(frac=1), [int(0.6 * len(df)), int(0.8 *len(df))])

def scale_dataset(dataframe):
  x = dataframe[dataframe.cols[:-1]].values  # all the data
  y = dataframe[dataframe.cols[-1]].values   # class field data

  scaler = StandardScaler()
  X = scaler.fit_transform(x)

  data = np.hstack((X, np.reshape(y, (-1,1))))

  return data, X, y

print(len(train[train['class'] == 1]))  # gamma
print(len(train[train['class'] == 0]))  # hadron

#  we want to over sample of train dataset as hadron is less then gamma value
#  we imported from imblearn.over_sampling import RandomOverSampler at top


# Rewrite the function again
def scale_dataset(dataframe, oversample = False):
  X = dataframe[dataframe.columns[:-1]].values  # all the data
  y = dataframe[dataframe.columns[-1]].values   # class field data

  scaler = StandardScaler()
  X = scaler.fit_transform(X)

  if oversample:
    ros = RandomOverSampler()
    X,y = ros.fit_resample(X, y)

  data = np.hstack((X, np.reshape(y, (-1,1))))

  return data, X, y

train, X_train, y_train = scale_dataset(train, oversample = True)
valid, X_valid, y_valid = scale_dataset(valid, oversample = False)
test, X_test, y_test = scale_dataset(test, oversample = False)

print(sum(y_train == 1))
print(sum(y_train == 0)) # now both gamma and hadron are evenly equal

"""# K-nearest Neighbour Algorithm"""

from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import classification_report

knn_model = KNeighborsClassifier(n_neighbors=5)

# train the model
knn_model.fit(X_train, y_train)

# Predict the data using model
y_pred = knn_model.predict(X_test)

print(classification_report(y_test, y_pred))

"""# Naive Bayes"""

from sklearn.naive_bayes import GaussianNB

nb_model = GaussianNB()

# Train the model
nb_model.fit(X_train, y_train)

# predict the model
y_pred = nb_model.predict(X_test)
print(classification_report(y_test, y_pred))

"""Navie Bayes theorm is more wost

# Logistic Regression
"""

from sklearn.linear_model import LogisticRegression

lg_model = LogisticRegression()

# Train the model
lg_model.fit(X_train, y_train)

# Predict the model
y_pred = lg_model.predict(X_test)
print(classification_report(y_test, y_pred))

"""Logistic Regression is better than navie bayes theorem but not better than k-nearest neighbour

# SVM
"""

from sklearn.svm import SVC

svm_model = SVC()

# Train the model
svm_model = svm_model.fit(X_train, y_train)

# Predict the model
y_pred = svm_model.predict(X_test)
print(classification_report(y_test, y_pred))

"""SVM model is more better then all above models

# Neural Network
"""

import tensorflow as tf

def plot_history(history):
  fig, (ax1, ax2) = plt.subplots(1,2, figsize=(10, 4))
  ax1.plot(history.history['loss'], label='loss')
  ax1.plot(history.history['val_loss'], label='val_loss')
  ax1.set_xlabel("Epoch")
  ax1.set_ylabel("Binary Crossentropy")
  ax1.grid(True)

  ax2.plot(history.history['accuracy'], label='accuracy')
  ax2.plot(history.history['val_accuracy'], label='val_accuracy')
  ax2.set_xlabel("Epoch")
  ax2.set_ylabel("Accuracy")
  ax2.grid(True)

  plt.show()

def train_model(X_train, y_train, num_nodes, dropout_prob, lr, batch_size, epochs):
  nn_model = tf.keras.Sequential([
      tf.keras.layers.Dense(num_nodes, activation='relu', input_shape=(10,)),
      tf.keras.layers.Dropout(dropout_prob),
      tf.keras.layers.Dense(num_nodes, activation='relu'),
      tf.keras.layers.Dropout(dropout_prob),
      tf.keras.layers.Dense(1, activation='sigmoid')
  ])

  nn_model.compile(optimizer=tf.keras.optimizers.Adam(lr), loss='binary_crossentropy',
                  metrics=['accuracy'])
  history = nn_model.fit(
    X_train, y_train, epochs=epochs, batch_size=batch_size, validation_split=0.2, verbose=0
  )

  return nn_model, history

least_val_loss = float('inf')
least_loss_model = None

epochs = 100
for num_nodes in [16, 32, 64]:
    for dropout_prob in [0, 0.2]:
        for lr in [0.01, 0.005, 0.001]:
            for batch_size in [32, 64, 128]:
                print(f"{num_nodes} nodes, dropout {dropout_prob}, lr = {lr}, batch_size = {batch_size}")
                model, history = train_model(X_train, y_train, num_nodes, dropout_prob, lr, batch_size, epochs)
                plot_history(history)

                val_loss = model.evaluate(X_valid, y_valid)[0]

                if val_loss < least_val_loss:
                    least_val_loss = val_loss
                    least_loss_model = model

# predict the data
y_pred = least_loss_model.predict(X_test)
y_pred

y_pred = (y_pred > 0.5).astype(int)
y_pred

# reshape the predicted values in one dimension
y_pred = (y_pred > 0.5).astype(int).reshape(-1, )

y_pred

print(classification_report(y_test, y_pred))

